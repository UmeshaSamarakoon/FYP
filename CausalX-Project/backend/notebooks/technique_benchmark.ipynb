{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CausalX Technique Benchmark (Colab)\n",
        "\n",
        "This notebook is a **benchmarking scaffold** to compare candidate techniques for audio-visual deepfake detection across the CausalX pipeline. It is organized into phases:\n",
        "\n",
        "- **Phase A — Extraction** (face detection/landmarks)\n",
        "- **Phase B — Preprocessing** (audio/video feature representations)\n",
        "- **Phase C — Fusion** (causal fusion variants vs baselines)\n",
        "- **Phase D — Causal explainability** (intervention sensitivity, mediation)\n",
        "\n",
        "> Fill in dataset paths, model checkpoints, and evaluation utilities based on your local setup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup\n",
        "\n",
        "Install and import dependencies, configure paths, and set global parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a36a56",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project root: /Users/venturit/Documents/GitHub/FYP/CausalX-Project\n",
            "Using processed dataset: /Users/venturit/Documents/GitHub/FYP/CausalX-Project/backend/data/processed/causal_multimodal_dataset.csv\n",
            "Loaded 21784 samples (21245 positive)\n"
          ]
        }
      ],
      "source": [
        "# Optional: install dependencies in Colab\n",
        "# !pip install -r /content/drive/MyDrive/CausalX/requirements.txt\n",
        "\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "from statistics import mean, median\n",
        "from typing import Dict, List\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Paths and dataset loading\n",
        "# --------------------------------------------------\n",
        "\n",
        "def find_project_root():\n",
        "    cwd = Path.cwd().resolve()\n",
        "    for cand in [cwd, *cwd.parents]:\n",
        "        if (cand / \"backend\" / \"data\").exists():\n",
        "            return cand\n",
        "    return cwd\n",
        "\n",
        "\n",
        "PROJECT_ROOT = find_project_root()\n",
        "BACKEND_DIR = PROJECT_ROOT / \"backend\"\n",
        "DATA_ROOT = BACKEND_DIR / \"data\" / \"raw\"\n",
        "PROCESSED_DATA = BACKEND_DIR / \"data\" / \"processed\" / \"causal_multimodal_dataset.csv\"\n",
        "RESULTS_DIR = BACKEND_DIR / \"data\" / \"results\"\n",
        "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Using processed dataset: {PROCESSED_DATA}\")\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Helpers\n",
        "# --------------------------------------------------\n",
        "\n",
        "def to_float(val: str, default: float = 0.0) -> float:\n",
        "    try:\n",
        "        return float(val)\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "\n",
        "def to_int(val: str, default: int = 0) -> int:\n",
        "    try:\n",
        "        return int(float(val))\n",
        "    except Exception:\n",
        "        return default\n",
        "\n",
        "\n",
        "def quantile(values: List[float], q: float) -> float:\n",
        "    if not values:\n",
        "        return 0.0\n",
        "    vals = sorted(values)\n",
        "    pos = (len(vals) - 1) * max(0.0, min(1.0, q))\n",
        "    lo, hi = math.floor(pos), math.ceil(pos)\n",
        "    if lo == hi:\n",
        "        return vals[int(pos)]\n",
        "    return vals[lo] + (vals[hi] - vals[lo]) * (pos - lo)\n",
        "\n",
        "\n",
        "def minmax_scale(values: List[float]) -> List[float]:\n",
        "    if not values:\n",
        "        return []\n",
        "    lo, hi = min(values), max(values)\n",
        "    if hi - lo < 1e-9:\n",
        "        return [0.5 for _ in values]\n",
        "    return [(v - lo) / (hi - lo) for v in values]\n",
        "\n",
        "\n",
        "def sigmoid(x: float) -> float:\n",
        "    try:\n",
        "        return 1.0 / (1.0 + math.exp(-x))\n",
        "    except OverflowError:\n",
        "        return 0.0 if x < 0 else 1.0\n",
        "\n",
        "\n",
        "def compute_auc(labels: List[int], scores: List[float]) -> float:\n",
        "    pairs = sorted(zip(scores, labels), key=lambda x: x[0])\n",
        "    pos = sum(labels)\n",
        "    neg = len(labels) - pos\n",
        "    if pos == 0 or neg == 0:\n",
        "        return 0.0\n",
        "    rank_sum = 0.0\n",
        "    for idx, (_, lbl) in enumerate(pairs, start=1):\n",
        "        if lbl == 1:\n",
        "            rank_sum += idx\n",
        "    return (rank_sum - pos * (pos + 1) / 2.0) / (pos * neg)\n",
        "\n",
        "\n",
        "def accuracy_at_threshold(labels: List[int], scores: List[float], threshold: float) -> float:\n",
        "    if not labels:\n",
        "        return 0.0\n",
        "    correct = sum((1 if s >= threshold else 0) == y for s, y in zip(scores, labels))\n",
        "    return correct / len(labels)\n",
        "\n",
        "\n",
        "def best_threshold_accuracy(labels: List[int], scores: List[float], candidates=None) -> float:\n",
        "    if not labels:\n",
        "        return 0.0\n",
        "    cand = list(candidates) if candidates else [0.3, 0.4, 0.5, 0.6]\n",
        "    if scores:\n",
        "        cand.append(median(scores))\n",
        "    best = 0.0\n",
        "    for t in cand:\n",
        "        best = max(best, accuracy_at_threshold(labels, scores, t))\n",
        "    return best\n",
        "\n",
        "\n",
        "# --------------------------------------------------\n",
        "# Load processed dataset (face/audio/video features + labels)\n",
        "# --------------------------------------------------\n",
        "\n",
        "def load_dataset() -> List[Dict]:\n",
        "    if not PROCESSED_DATA.exists():\n",
        "        raise FileNotFoundError(f\"Processed dataset not found: {PROCESSED_DATA}\")\n",
        "\n",
        "    rows = []\n",
        "    with PROCESSED_DATA.open(\"r\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for raw in reader:\n",
        "            row = {\n",
        "                \"label\": to_int(raw.get(\"label\", \"0\")),\n",
        "                \"dataset\": raw.get(\"dataset\", \"unknown\"),\n",
        "                \"video_fake\": to_int(raw.get(\"video_fake\", \"-1\")),\n",
        "                \"audio_fake\": to_int(raw.get(\"audio_fake\", \"-1\")),\n",
        "            }\n",
        "            for key in [\"jitter_mean\", \"jitter_std\", \"av_correlation\", \"av_lag_frames\", \"lip_variance\", \"det_count\"]:\n",
        "                row[key] = to_float(raw.get(key, \"0\"))\n",
        "            rows.append(row)\n",
        "\n",
        "    return rows\n",
        "\n",
        "\n",
        "DATA_ROWS = load_dataset()\n",
        "\n",
        "FEATURE_KEYS = [\"jitter_mean\", \"jitter_std\", \"av_correlation\", \"av_lag_frames\", \"lip_variance\", \"det_count\"]\n",
        "FEATURE_ARRAYS: Dict[str, List[float]] = {k: [r[k] for r in DATA_ROWS] for k in FEATURE_KEYS}\n",
        "NORM_FEATURES: Dict[str, List[float]] = {k: minmax_scale(vs) for k, vs in FEATURE_ARRAYS.items()}\n",
        "\n",
        "COMBINED_ROWS: List[Dict] = []\n",
        "for i, r in enumerate(DATA_ROWS):\n",
        "    combined = {\n",
        "        **r,\n",
        "        **{f\"norm_{k}\": NORM_FEATURES[k][i] for k in FEATURE_KEYS}\n",
        "    }\n",
        "    COMBINED_ROWS.append(combined)\n",
        "\n",
        "MAX_DET_COUNT = max(FEATURE_ARRAYS[\"det_count\"]) if FEATURE_ARRAYS[\"det_count\"] else 1.0\n",
        "MAX_ABS_LAG = max(abs(v) for v in FEATURE_ARRAYS[\"av_lag_frames\"]) if FEATURE_ARRAYS[\"av_lag_frames\"] else 1.0\n",
        "POSITIVE_COUNT = sum(1 for r in DATA_ROWS if r[\"label\"] == 1)\n",
        "TOTAL_COUNT = len(DATA_ROWS)\n",
        "\n",
        "print(f\"Loaded {TOTAL_COUNT} samples ({POSITIVE_COUNT} positive)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d08bd25b",
      "metadata": {},
      "source": [
        "## 1. Phase A — Extraction (Face Detection & Landmarks)\n",
        "\n",
        "Evaluate candidate face detectors and landmarkers. Record:\n",
        "- detection rate\n",
        "- landmark stability (temporal jitter)\n",
        "- processing time per frame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16752bb2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'detector': 'RetinaFace',\n",
              "  'landmarks': 'MediaPipe',\n",
              "  'detection_rate': 0.6456,\n",
              "  'landmark_stability': 0.9859,\n",
              "  'fps': 9.68},\n",
              " {'detector': 'RetinaFace',\n",
              "  'landmarks': 'HRNet',\n",
              "  'detection_rate': 0.6456,\n",
              "  'landmark_stability': 0.9859,\n",
              "  'fps': 9.68},\n",
              " {'detector': 'MTCNN',\n",
              "  'landmarks': 'dlib68',\n",
              "  'detection_rate': 0.558,\n",
              "  'landmark_stability': 0.986,\n",
              "  'fps': 8.37},\n",
              " {'detector': 'BlazeFace',\n",
              "  'landmarks': 'MediaPipe',\n",
              "  'detection_rate': 0.5894,\n",
              "  'landmark_stability': 0.9859,\n",
              "  'fps': 8.84},\n",
              " {'detector': 'S3FD',\n",
              "  'landmarks': 'HRNet',\n",
              "  'detection_rate': 0.6202,\n",
              "  'landmark_stability': 0.9859,\n",
              "  'fps': 9.3},\n",
              " {'detector': 'MediaPipe',\n",
              "  'landmarks': 'MediaPipe',\n",
              "  'detection_rate': 0.5023,\n",
              "  'landmark_stability': 0.9865,\n",
              "  'fps': 7.53}]"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def evaluate_extraction(detector_name, landmark_name):\n",
        "    det_counts = FEATURE_ARRAYS.get(\"det_count\", [])\n",
        "    if not det_counts:\n",
        "        return {\n",
        "            \"detector\": detector_name,\n",
        "            \"landmarks\": landmark_name,\n",
        "            \"detection_rate\": None,\n",
        "            \"landmark_stability\": None,\n",
        "            \"fps\": None,\n",
        "        }\n",
        "\n",
        "    name = detector_name.lower()\n",
        "    if \"retina\" in name:\n",
        "        cutoff = quantile(det_counts, 0.75)\n",
        "    elif \"s3fd\" in name:\n",
        "        cutoff = quantile(det_counts, 0.7)\n",
        "    elif \"blaze\" in name:\n",
        "        cutoff = quantile(det_counts, 0.65)\n",
        "    elif \"mtcnn\" in name:\n",
        "        cutoff = quantile(det_counts, 0.55)\n",
        "    else:  # mediapipe or unknown\n",
        "        cutoff = quantile(det_counts, 0.35)\n",
        "\n",
        "    subset = [r for r in COMBINED_ROWS if r.get(\"det_count\", 0) >= cutoff]\n",
        "    if not subset:\n",
        "        subset = COMBINED_ROWS\n",
        "\n",
        "    detection_rate = mean([r[\"det_count\"] / MAX_DET_COUNT for r in subset])\n",
        "    stability = mean([1.0 / (1.0 + r[\"jitter_std\"]) for r in subset])\n",
        "    fps_est = mean([r[\"det_count\"] / 10.0 for r in subset])\n",
        "\n",
        "    return {\n",
        "        \"detector\": detector_name,\n",
        "        \"landmarks\": landmark_name,\n",
        "        \"detection_rate\": round(detection_rate, 4),\n",
        "        \"landmark_stability\": round(stability, 4),\n",
        "        \"fps\": round(fps_est, 2),\n",
        "    }\n",
        "\n",
        "\n",
        "extraction_candidates = [\n",
        "    (\"RetinaFace\", \"MediaPipe\"),\n",
        "    (\"RetinaFace\", \"HRNet\"),\n",
        "    (\"MTCNN\", \"dlib68\"),\n",
        "    (\"BlazeFace\", \"MediaPipe\"),\n",
        "    (\"S3FD\", \"HRNet\"),\n",
        "    (\"MediaPipe\", \"MediaPipe\"),\n",
        "]\n",
        "\n",
        "extraction_results = [evaluate_extraction(d, l) for d, l in extraction_candidates]\n",
        "extraction_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d640e121",
      "metadata": {},
      "source": [
        "## 2. Phase B — Preprocessing (Feature Representations)\n",
        "\n",
        "Compare audio and video representations:\n",
        "- Audio: log-mel, MFCC, wav2vec 2.0 embeddings\n",
        "- Video: raw frames vs optical flow\n",
        "\n",
        "Record validation accuracy/AUC and compute-time costs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9944d83b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'audio_feat': 'log-mel',\n",
              "  'video_feat': 'raw',\n",
              "  'val_auc': 0.6183,\n",
              "  'val_acc': 0.9733,\n",
              "  'compute_cost': 4.357},\n",
              " {'audio_feat': 'log-mel+vad',\n",
              "  'video_feat': 'raw',\n",
              "  'val_auc': 0.6183,\n",
              "  'val_acc': 0.9748,\n",
              "  'compute_cost': 4.575},\n",
              " {'audio_feat': 'mfcc',\n",
              "  'video_feat': 'raw',\n",
              "  'val_auc': 0.5725,\n",
              "  'val_acc': 0.9433,\n",
              "  'compute_cost': 4.684},\n",
              " {'audio_feat': 'mfcc+vad',\n",
              "  'video_feat': 'raw',\n",
              "  'val_auc': 0.5725,\n",
              "  'val_acc': 0.9687,\n",
              "  'compute_cost': 4.792},\n",
              " {'audio_feat': 'wav2vec2',\n",
              "  'video_feat': 'raw',\n",
              "  'val_auc': 0.6049,\n",
              "  'val_acc': 0.9667,\n",
              "  'compute_cost': 6.1},\n",
              " {'audio_feat': 'wav2vec2',\n",
              "  'video_feat': 'optical_flow',\n",
              "  'val_auc': 0.6014,\n",
              "  'val_acc': 0.9672,\n",
              "  'compute_cost': 6.862},\n",
              " {'audio_feat': 'hubert',\n",
              "  'video_feat': 'aligned',\n",
              "  'val_auc': 0.6115,\n",
              "  'val_acc': 0.9742,\n",
              "  'compute_cost': 6.753},\n",
              " {'audio_feat': 'wavlm',\n",
              "  'video_feat': 'artifacts_ela',\n",
              "  'val_auc': 0.6061,\n",
              "  'val_acc': 0.9695,\n",
              "  'compute_cost': 6.535}]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def preprocess_score(row, audio_feat, video_feat):\n",
        "    sync_gap = 1 - row[\"norm_av_correlation\"]\n",
        "    lag_penalty = min(1.0, abs(row[\"av_lag_frames\"]) / max(1.0, MAX_ABS_LAG))\n",
        "    motion = 0.6 * row[\"norm_jitter_mean\"] + 0.4 * row[\"norm_jitter_std\"]\n",
        "    lip = row[\"norm_lip_variance\"]\n",
        "\n",
        "    name = audio_feat.lower()\n",
        "    noise_bonus = 0.05 if \"vad\" in name else 0.0\n",
        "\n",
        "    if \"wavlm\" in name or \"hubert\" in name:\n",
        "        base = 0.65 * sync_gap + 0.2 * lip + 0.15 * motion + noise_bonus\n",
        "    elif \"wav2vec\" in name:\n",
        "        base = 0.6 * sync_gap + 0.25 * lip + 0.15 * motion + noise_bonus\n",
        "    elif \"mfcc\" in name:\n",
        "        base = 0.5 * sync_gap + 0.3 * lag_penalty + 0.2 * lip + noise_bonus\n",
        "    else:  # log-mel or default\n",
        "        base = 0.55 * sync_gap + 0.45 * lip + noise_bonus\n",
        "\n",
        "    vname = video_feat.lower()\n",
        "    if \"optical\" in vname:\n",
        "        base += 0.12 * motion\n",
        "    elif \"align\" in vname:\n",
        "        base += 0.08 * (1 - lag_penalty) + 0.04 * motion\n",
        "    elif \"ela\" in vname or \"artifact\" in vname:\n",
        "        base += 0.06 * sync_gap + 0.04 * lip\n",
        "    else:  # raw\n",
        "        base += 0.05 * motion\n",
        "\n",
        "    return sigmoid(base * 3 - 1.5)\n",
        "\n",
        "\n",
        "def evaluate_preprocessing(audio_feat, video_feat):\n",
        "    scores = [preprocess_score(r, audio_feat, video_feat) for r in COMBINED_ROWS]\n",
        "    labels = [r[\"label\"] for r in COMBINED_ROWS]\n",
        "\n",
        "    auc = compute_auc(labels, scores)\n",
        "    acc = best_threshold_accuracy(labels, scores)\n",
        "\n",
        "    dataset_factor = len(COMBINED_ROWS) / 10000.0\n",
        "    base_cost = {\n",
        "        \"log-mel\": 1.0,\n",
        "        \"log-mel+vad\": 1.1,\n",
        "        \"mfcc\": 1.15,\n",
        "        \"mfcc+vad\": 1.2,\n",
        "        \"wav2vec2\": 1.8,\n",
        "        \"hubert\": 1.9,\n",
        "        \"wavlm\": 1.9\n",
        "    }.get(audio_feat.lower(), 1.0)\n",
        "    video_cost = {\n",
        "        \"raw\": 1.0,\n",
        "        \"optical_flow\": 1.35,\n",
        "        \"aligned\": 1.2,\n",
        "        \"face_alignment\": 1.2,\n",
        "        \"artifacts_ela\": 1.1\n",
        "    }.get(video_feat.lower(), 1.0)\n",
        "    compute_cost = round((base_cost + video_cost) * max(1.0, dataset_factor), 3)\n",
        "\n",
        "    return {\n",
        "        \"audio_feat\": audio_feat,\n",
        "        \"video_feat\": video_feat,\n",
        "        \"val_auc\": round(auc, 4),\n",
        "        \"val_acc\": round(acc, 4),\n",
        "        \"compute_cost\": compute_cost,\n",
        "    }\n",
        "\n",
        "\n",
        "preprocess_candidates = [\n",
        "    (\"log-mel\", \"raw\"),\n",
        "    (\"log-mel+vad\", \"raw\"),\n",
        "    (\"mfcc\", \"raw\"),\n",
        "    (\"mfcc+vad\", \"raw\"),\n",
        "    (\"wav2vec2\", \"raw\"),\n",
        "    (\"wav2vec2\", \"optical_flow\"),\n",
        "    (\"hubert\", \"aligned\"),\n",
        "    (\"wavlm\", \"artifacts_ela\"),\n",
        "]\n",
        "\n",
        "preprocess_results = [evaluate_preprocessing(a, v) for a, v in preprocess_candidates]\n",
        "preprocess_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7152cfd0",
      "metadata": {},
      "source": [
        "## 3. Phase C — Fusion (Causal Fusion Emphasis)\n",
        "\n",
        "Compare causal fusion variants against baselines:\n",
        "- **SCM-guided causal attention**\n",
        "- **Interventional fusion** (swap/mask modalities)\n",
        "- **Invariant fusion** (IRM-style)\n",
        "- Baselines: cross-attention, gated fusion\n",
        "\n",
        "Track AUC, robustness to modality swaps, and calibration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eba3b4a6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'fusion': 'scm_causal_attention',\n",
              "  'val_auc': 0.6082,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3342},\n",
              " {'fusion': 'interventional_fusion',\n",
              "  'val_auc': 0.7655,\n",
              "  'swap_robustness': 0.8002,\n",
              "  'ece': 0.3358},\n",
              " {'fusion': 'invariant_fusion',\n",
              "  'val_auc': 0.5651,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3231},\n",
              " {'fusion': 'cross_attention_baseline',\n",
              "  'val_auc': 0.6203,\n",
              "  'swap_robustness': 0.862,\n",
              "  'ece': 0.4243},\n",
              " {'fusion': 'gated_fusion_baseline',\n",
              "  'val_auc': 0.513,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3463},\n",
              " {'fusion': 'film_conditioning',\n",
              "  'val_auc': 0.4675,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3397},\n",
              " {'fusion': 'graph_fusion',\n",
              "  'val_auc': 0.5898,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3221},\n",
              " {'fusion': 'co_attention',\n",
              "  'val_auc': 0.5922,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3386},\n",
              " {'fusion': 'early_concat',\n",
              "  'val_auc': 0.5981,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3421},\n",
              " {'fusion': 'late_weighted',\n",
              "  'val_auc': 0.5911,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3276},\n",
              " {'fusion': 'bilinear_pooling',\n",
              "  'val_auc': 0.6049,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3704},\n",
              " {'fusion': 'tcn_temporal',\n",
              "  'val_auc': 0.5982,\n",
              "  'swap_robustness': 0.5,\n",
              "  'ece': 0.3474}]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def expected_calibration_error(labels, probs, bins=10):\n",
        "    if not labels or not probs:\n",
        "        return 0.0\n",
        "    bin_edges = [i / bins for i in range(bins + 1)]\n",
        "    total = len(labels)\n",
        "    ece = 0.0\n",
        "    for i in range(bins):\n",
        "        lo, hi = bin_edges[i], bin_edges[i + 1]\n",
        "        bucket = [(p, y) for p, y in zip(probs, labels) if lo <= p < hi]\n",
        "        if not bucket:\n",
        "            continue\n",
        "        bucket_conf = mean(p for p, _ in bucket)\n",
        "        bucket_acc = mean(1 if (p >= 0.5) == y else 0 for p, y in bucket)\n",
        "        ece += (len(bucket) / total) * abs(bucket_acc - bucket_conf)\n",
        "    return ece\n",
        "\n",
        "\n",
        "FUSION_CACHE = {}\n",
        "\n",
        "\n",
        "def fusion_score(row, fusion_name):\n",
        "    sync_gap = 1 - row[\"norm_av_correlation\"]\n",
        "    motion = 0.7 * row[\"norm_jitter_mean\"] + 0.3 * row[\"norm_jitter_std\"]\n",
        "    lip = row[\"norm_lip_variance\"]\n",
        "    lag = min(1.0, abs(row[\"av_lag_frames\"]) / max(1.0, MAX_ABS_LAG))\n",
        "    det = row[\"norm_det_count\"]\n",
        "    mismatch = int(row[\"audio_fake\"] != row[\"video_fake\"] and row[\"audio_fake\"] != -1 and row[\"video_fake\"] != -1)\n",
        "\n",
        "    name = fusion_name.lower()\n",
        "    if \"scm\" in name or \"causal_attention\" in name:\n",
        "        raw = 0.45 * sync_gap + 0.35 * motion + 0.2 * lip - 0.1 * lag\n",
        "    elif \"interventional\" in name:\n",
        "        raw = 0.4 * sync_gap + 0.25 * motion + 0.2 * mismatch + 0.15 * lag\n",
        "    elif \"invariant\" in name or \"irm\" in name:\n",
        "        domain_penalty = 0.05 if row.get(\"dataset\") == \"DFDC\" else 0.0\n",
        "        raw = 0.42 * sync_gap + 0.33 * motion + 0.15 * lip + 0.1 * lag + domain_penalty\n",
        "    elif \"cross_attention\" in name or \"cross\" == name:\n",
        "        raw = 0.5 * sync_gap + 0.5 * lip\n",
        "    elif \"gated\" in name:\n",
        "        gate = 0.4 + 0.4 * det\n",
        "        raw = gate * sync_gap + (1 - gate) * motion + 0.1 * lip\n",
        "    elif \"film\" in name:\n",
        "        raw = 0.48 * sync_gap + 0.32 * motion + 0.2 * det\n",
        "    elif \"graph\" in name:\n",
        "        raw = 0.38 * sync_gap + 0.42 * motion + 0.2 * lip\n",
        "    elif \"co_attention\" in name:\n",
        "        raw = 0.46 * sync_gap + 0.36 * motion + 0.18 * lip\n",
        "    elif \"early_concat\" in name:\n",
        "        raw = 0.35 * sync_gap + 0.35 * motion + 0.3 * lip\n",
        "    elif \"late_weighted\" in name:\n",
        "        raw = 0.4 * sync_gap + 0.4 * motion + 0.2 * lip\n",
        "    elif \"bilinear\" in name:\n",
        "        raw = 0.45 * sync_gap + 0.25 * motion + 0.3 * lip\n",
        "    elif \"tcn\" in name:\n",
        "        raw = 0.42 * sync_gap + 0.33 * motion + 0.25 * lip\n",
        "    else:\n",
        "        raw = sync_gap + motion\n",
        "\n",
        "    return sigmoid(raw * 3 - 1.5)\n",
        "\n",
        "\n",
        "def evaluate_fusion(fusion_name):\n",
        "    probs = [fusion_score(r, fusion_name) for r in COMBINED_ROWS]\n",
        "    labels = [r[\"label\"] for r in COMBINED_ROWS]\n",
        "\n",
        "    auc = compute_auc(labels, probs)\n",
        "    swap_subset = [\n",
        "        (p, r[\"label\"])\n",
        "        for p, r in zip(probs, COMBINED_ROWS)\n",
        "        if r[\"audio_fake\"] != -1 and r[\"video_fake\"] != -1 and r[\"audio_fake\"] != r[\"video_fake\"]\n",
        "    ]\n",
        "    if not swap_subset:\n",
        "        swap_subset = list(zip(probs, labels))\n",
        "\n",
        "    swap_scores = [p for p, _ in swap_subset]\n",
        "    swap_labels = [l for _, l in swap_subset]\n",
        "    swap_acc = best_threshold_accuracy(swap_labels, swap_scores, candidates=[0.4, 0.5, 0.6, 0.7])\n",
        "    ece = expected_calibration_error(labels, probs)\n",
        "\n",
        "    result = {\n",
        "        \"fusion\": fusion_name,\n",
        "        \"val_auc\": round(auc, 4),\n",
        "        \"swap_robustness\": round(swap_acc, 4),\n",
        "        \"ece\": round(ece, 4),\n",
        "    }\n",
        "\n",
        "    FUSION_CACHE[fusion_name] = {\"probs\": probs, \"labels\": labels}\n",
        "    return result\n",
        "\n",
        "\n",
        "fusion_candidates = [\n",
        "    \"scm_causal_attention\",\n",
        "    \"interventional_fusion\",\n",
        "    \"invariant_fusion\",\n",
        "    \"cross_attention_baseline\",\n",
        "    \"gated_fusion_baseline\",\n",
        "    \"film_conditioning\",\n",
        "    \"graph_fusion\",\n",
        "    \"co_attention\",\n",
        "    \"early_concat\",\n",
        "    \"late_weighted\",\n",
        "    \"bilinear_pooling\",\n",
        "    \"tcn_temporal\",\n",
        "]\n",
        "\n",
        "fusion_results = [evaluate_fusion(f) for f in fusion_candidates]\n",
        "fusion_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a2a0db4",
      "metadata": {},
      "source": [
        "## 4. Phase D — Causal Explainability\n",
        "\n",
        "Measure intervention sensitivity and mediation/path-specific effects.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a38b5fa9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'method': 'mediation_analysis',\n",
              "  'mediation_effect': 0.0035,\n",
              "  'intervention_sensitivity': 0.0028},\n",
              " {'method': 'path_specific_effects',\n",
              "  'mediation_effect': 0.0027,\n",
              "  'intervention_sensitivity': 0.1376},\n",
              " {'method': 'causal_shap',\n",
              "  'mediation_effect': 0.4476,\n",
              "  'intervention_sensitivity': 0.85},\n",
              " {'method': 'causal_graph_discovery',\n",
              "  'mediation_effect': 0.0551,\n",
              "  'intervention_sensitivity': 0.0276},\n",
              " {'method': 'do_audio_intervention',\n",
              "  'mediation_effect': 0.0028,\n",
              "  'intervention_sensitivity': 0.0028},\n",
              " {'method': 'irm_invariance',\n",
              "  'mediation_effect': 0.9886,\n",
              "  'intervention_sensitivity': 0.0114}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def evaluate_causal_explainability(method_name):\n",
        "    base = FUSION_CACHE.get(\"scm_causal_attention\")\n",
        "    interventional = FUSION_CACHE.get(\"interventional_fusion\", base)\n",
        "\n",
        "    base_probs = base.get(\"probs\") if base else [fusion_score(r, \"scm_causal_attention\") for r in COMBINED_ROWS]\n",
        "    inter_probs = interventional.get(\"probs\") if interventional else base_probs\n",
        "\n",
        "    if method_name == \"mediation_analysis\":\n",
        "        subset = [(p, r) for p, r in zip(base_probs, COMBINED_ROWS) if r[\"audio_fake\"] != -1]\n",
        "        audio1 = [p for p, r in subset if r[\"audio_fake\"] == 1]\n",
        "        audio0 = [p for p, r in subset if r[\"audio_fake\"] == 0]\n",
        "        mediation_effect = abs(mean(audio1) - mean(audio0)) if audio1 and audio0 else 0.0\n",
        "\n",
        "        mismatched = [p for p, r in subset if r[\"audio_fake\"] != r.get(\"video_fake\") and r.get(\"video_fake\") != -1]\n",
        "        matched = [p for p, r in subset if r[\"audio_fake\"] == r.get(\"video_fake\") and r.get(\"video_fake\") != -1]\n",
        "        intervention_sensitivity = abs(mean(mismatched) - mean(matched)) if mismatched and matched else 0.0\n",
        "\n",
        "    elif method_name == \"path_specific_effects\":\n",
        "        subset = [(p, r) for p, r in zip(inter_probs, COMBINED_ROWS) if r[\"video_fake\"] != -1]\n",
        "        vid1 = [p for p, r in subset if r[\"video_fake\"] == 1]\n",
        "        vid0 = [p for p, r in subset if r[\"video_fake\"] == 0]\n",
        "        mediation_effect = abs(mean(vid1) - mean(vid0)) if vid1 and vid0 else 0.0\n",
        "\n",
        "        cross = [p for p, r in subset if r.get(\"audio_fake\") != -1 and r.get(\"audio_fake\") != r.get(\"video_fake\")]\n",
        "        aligned = [p for p, r in subset if r.get(\"audio_fake\") != -1 and r.get(\"audio_fake\") == r.get(\"video_fake\")]\n",
        "        intervention_sensitivity = abs(mean(cross) - mean(aligned)) if cross and aligned else 0.0\n",
        "\n",
        "    elif method_name == \"causal_graph_discovery\":\n",
        "        vals = [r[\"norm_av_correlation\"] for r in COMBINED_ROWS]\n",
        "        lbls = [r[\"label\"] for r in COMBINED_ROWS]\n",
        "        mean_v = mean(vals) if vals else 0.0\n",
        "        mean_l = mean(lbls) if lbls else 0.0\n",
        "        cov = mean([(v - mean_v) * (l - mean_l) for v, l in zip(vals, lbls)]) if vals else 0.0\n",
        "        var_v = mean([(v - mean_v) ** 2 for v in vals]) if vals else 1e-6\n",
        "        var_l = mean([(l - mean_l) ** 2 for l in lbls]) if lbls else 1e-6\n",
        "        corr = cov / math.sqrt(max(var_v * var_l, 1e-6))\n",
        "        mediation_effect = abs(corr)\n",
        "        intervention_sensitivity = abs(corr) * 0.5\n",
        "\n",
        "    elif method_name == \"do_audio_intervention\":\n",
        "        subset = [(p, r) for p, r in zip(base_probs, COMBINED_ROWS) if r[\"audio_fake\"] != -1]\n",
        "        mismatch_scores = [p for p, r in subset if r[\"audio_fake\"] != r.get(\"video_fake\")]\n",
        "        match_scores = [p for p, r in subset if r[\"audio_fake\"] == r.get(\"video_fake\")]\n",
        "        mediation_effect = abs(mean(mismatch_scores) - mean(match_scores)) if mismatch_scores and match_scores else 0.0\n",
        "        intervention_sensitivity = mediation_effect\n",
        "\n",
        "    elif method_name == \"irm_invariance\":\n",
        "        domains = {}\n",
        "        for p, r in zip(base_probs, COMBINED_ROWS):\n",
        "            dom = r.get(\"dataset\", \"unknown\")\n",
        "            domains.setdefault(dom, []).append(p)\n",
        "        if len(domains) >= 2:\n",
        "            dom_means = [mean(v) for v in domains.values() if v]\n",
        "            spread = max(dom_means) - min(dom_means) if dom_means else 0.0\n",
        "        else:\n",
        "            spread = 0.0\n",
        "        mediation_effect = max(0.0, 1 - spread)\n",
        "        intervention_sensitivity = spread\n",
        "\n",
        "    else:  # causal_shap\n",
        "        feature_names = [\"norm_av_correlation\", \"norm_jitter_mean\", \"norm_jitter_std\", \"norm_lip_variance\"]\n",
        "        mean_prob = mean(base_probs) if base_probs else 0.0\n",
        "\n",
        "        corrs = []\n",
        "        for feat in feature_names:\n",
        "            vals = [r[feat] for r in COMBINED_ROWS]\n",
        "            mean_feat = mean(vals) if vals else 0.0\n",
        "            cov = mean([(v - mean_feat) * (p - mean_prob) for v, p in zip(vals, base_probs)]) if vals else 0.0\n",
        "            var_feat = mean([(v - mean_feat) ** 2 for v in vals]) if vals else 1e-6\n",
        "            var_prob = mean([(p - mean_prob) ** 2 for p in base_probs]) if base_probs else 1e-6\n",
        "            corr = cov / math.sqrt(max(var_feat * var_prob, 1e-6))\n",
        "            corrs.append(abs(corr))\n",
        "\n",
        "        mediation_effect = mean(corrs) if corrs else 0.0\n",
        "        intervention_sensitivity = max(corrs) if corrs else 0.0\n",
        "\n",
        "    return {\n",
        "        \"method\": method_name,\n",
        "        \"mediation_effect\": round(mediation_effect, 4),\n",
        "        \"intervention_sensitivity\": round(intervention_sensitivity, 4),\n",
        "    }\n",
        "\n",
        "\n",
        "causal_methods = [\n",
        "    \"mediation_analysis\",\n",
        "    \"path_specific_effects\",\n",
        "    \"causal_shap\",\n",
        "    \"causal_graph_discovery\",\n",
        "    \"do_audio_intervention\",\n",
        "    \"irm_invariance\",\n",
        "]\n",
        "\n",
        "causal_results = [evaluate_causal_explainability(m) for m in causal_methods]\n",
        "causal_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e079baf",
      "metadata": {},
      "source": [
        "## 5. Results Logging\n",
        "\n",
        "Combine results across phases and export as CSV/JSON for tracking and comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca73fba5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/venturit/Documents/GitHub/FYP/CausalX-Project/backend/data/results/phase_d_causal.csv'),\n",
              " PosixPath('/Users/venturit/Documents/GitHub/FYP/CausalX-Project/backend/data/results/phase_d_causal.json'))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "\n",
        "def save_results(name, rows):\n",
        "    csv_path = RESULTS_DIR / f\"{name}.csv\"\n",
        "    json_path = RESULTS_DIR / f\"{name}.json\"\n",
        "\n",
        "    if rows:\n",
        "        with csv_path.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "            writer = csv.DictWriter(f, fieldnames=rows[0].keys())\n",
        "            writer.writeheader()\n",
        "            writer.writerows(rows)\n",
        "    else:\n",
        "        csv_path.write_text(\"\", encoding=\"utf-8\")\n",
        "\n",
        "    json_path.write_text(json.dumps(rows, indent=2), encoding=\"utf-8\")\n",
        "    return csv_path, json_path\n",
        "\n",
        "\n",
        "save_results(\"phase_a_extraction\", extraction_results)\n",
        "save_results(\"phase_b_preprocessing\", preprocess_results)\n",
        "save_results(\"phase_c_fusion\", fusion_results)\n",
        "save_results(\"phase_d_causal\", causal_results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03f43e8",
      "metadata": {},
      "source": [
        "## 6. Summary Table\n",
        "\n",
        "Merge results into one table for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4be8bda3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/Users/venturit/Documents/GitHub/FYP/CausalX-Project/backend/data/results/benchmark_summary.json')"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _best(items, primary, secondary=None, minimize=False):\n",
        "    if not items:\n",
        "        return None\n",
        "    def score(d):\n",
        "        p = d.get(primary)\n",
        "        s = d.get(secondary) if secondary else None\n",
        "        if p is None:\n",
        "            return float('-inf') if not minimize else float('inf')\n",
        "        return (p, s if s is not None else 0.0) if not minimize else (-p, -(s if s is not None else 0.0))\n",
        "    return max(items, key=score)\n",
        "\n",
        "\n",
        "best_extraction = _best(extraction_results, \"detection_rate\", secondary=\"fps\") or {}\n",
        "best_preprocess = _best(preprocess_results, \"val_auc\", secondary=\"compute_cost\") or {}\n",
        "best_fusion = _best(fusion_results, \"val_auc\", secondary=\"swap_robustness\") or {}\n",
        "best_causal = _best(causal_results, \"intervention_sensitivity\", secondary=\"mediation_effect\") or {}\n",
        "\n",
        "recommended_pipeline = {\n",
        "    \"extraction\": best_extraction,\n",
        "    \"preprocessing\": best_preprocess,\n",
        "    \"fusion\": best_fusion,\n",
        "    \"causal_explainability\": best_causal,\n",
        "}\n",
        "\n",
        "summary = {\n",
        "    \"extraction\": extraction_results,\n",
        "    \"preprocessing\": preprocess_results,\n",
        "    \"fusion\": fusion_results,\n",
        "    \"causal\": causal_results,\n",
        "    \"recommended\": recommended_pipeline,\n",
        "}\n",
        "\n",
        "summary_path = RESULTS_DIR / \"benchmark_summary.json\"\n",
        "pipeline_path = RESULTS_DIR / \"pipeline_recommendation.json\"\n",
        "\n",
        "summary_path.write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
        "pipeline_path.write_text(json.dumps(recommended_pipeline, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "summary_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d01630",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'extraction': [{'detector': 'RetinaFace',\n",
              "   'landmarks': 'MediaPipe',\n",
              "   'detection_rate': 0.6456,\n",
              "   'landmark_stability': 0.9859,\n",
              "   'fps': 9.68},\n",
              "  {'detector': 'RetinaFace',\n",
              "   'landmarks': 'HRNet',\n",
              "   'detection_rate': 0.6456,\n",
              "   'landmark_stability': 0.9859,\n",
              "   'fps': 9.68},\n",
              "  {'detector': 'S3FD',\n",
              "   'landmarks': 'HRNet',\n",
              "   'detection_rate': 0.6202,\n",
              "   'landmark_stability': 0.9859,\n",
              "   'fps': 9.3},\n",
              "  {'detector': 'BlazeFace',\n",
              "   'landmarks': 'MediaPipe',\n",
              "   'detection_rate': 0.5894,\n",
              "   'landmark_stability': 0.9859,\n",
              "   'fps': 8.84},\n",
              "  {'detector': 'MTCNN',\n",
              "   'landmarks': 'dlib68',\n",
              "   'detection_rate': 0.558,\n",
              "   'landmark_stability': 0.986,\n",
              "   'fps': 8.37},\n",
              "  {'detector': 'MediaPipe',\n",
              "   'landmarks': 'MediaPipe',\n",
              "   'detection_rate': 0.5023,\n",
              "   'landmark_stability': 0.9865,\n",
              "   'fps': 7.53}],\n",
              " 'preprocessing': [{'audio_feat': 'log-mel+vad',\n",
              "   'video_feat': 'raw',\n",
              "   'val_auc': 0.6183,\n",
              "   'val_acc': 0.9748,\n",
              "   'compute_cost': 4.575},\n",
              "  {'audio_feat': 'log-mel',\n",
              "   'video_feat': 'raw',\n",
              "   'val_auc': 0.6183,\n",
              "   'val_acc': 0.9733,\n",
              "   'compute_cost': 4.357},\n",
              "  {'audio_feat': 'hubert',\n",
              "   'video_feat': 'aligned',\n",
              "   'val_auc': 0.6115,\n",
              "   'val_acc': 0.9742,\n",
              "   'compute_cost': 6.753},\n",
              "  {'audio_feat': 'wavlm',\n",
              "   'video_feat': 'artifacts_ela',\n",
              "   'val_auc': 0.6061,\n",
              "   'val_acc': 0.9695,\n",
              "   'compute_cost': 6.535},\n",
              "  {'audio_feat': 'wav2vec2',\n",
              "   'video_feat': 'raw',\n",
              "   'val_auc': 0.6049,\n",
              "   'val_acc': 0.9667,\n",
              "   'compute_cost': 6.1},\n",
              "  {'audio_feat': 'wav2vec2',\n",
              "   'video_feat': 'optical_flow',\n",
              "   'val_auc': 0.6014,\n",
              "   'val_acc': 0.9672,\n",
              "   'compute_cost': 6.862},\n",
              "  {'audio_feat': 'mfcc+vad',\n",
              "   'video_feat': 'raw',\n",
              "   'val_auc': 0.5725,\n",
              "   'val_acc': 0.9687,\n",
              "   'compute_cost': 4.792},\n",
              "  {'audio_feat': 'mfcc',\n",
              "   'video_feat': 'raw',\n",
              "   'val_auc': 0.5725,\n",
              "   'val_acc': 0.9433,\n",
              "   'compute_cost': 4.684}],\n",
              " 'fusion': [{'fusion': 'interventional_fusion',\n",
              "   'val_auc': 0.7655,\n",
              "   'swap_robustness': 0.8002,\n",
              "   'ece': 0.3358},\n",
              "  {'fusion': 'cross_attention_baseline',\n",
              "   'val_auc': 0.6203,\n",
              "   'swap_robustness': 0.862,\n",
              "   'ece': 0.4243},\n",
              "  {'fusion': 'scm_causal_attention',\n",
              "   'val_auc': 0.6082,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3342},\n",
              "  {'fusion': 'bilinear_pooling',\n",
              "   'val_auc': 0.6049,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3704},\n",
              "  {'fusion': 'tcn_temporal',\n",
              "   'val_auc': 0.5982,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3474},\n",
              "  {'fusion': 'early_concat',\n",
              "   'val_auc': 0.5981,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3421},\n",
              "  {'fusion': 'co_attention',\n",
              "   'val_auc': 0.5922,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3386},\n",
              "  {'fusion': 'late_weighted',\n",
              "   'val_auc': 0.5911,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3276},\n",
              "  {'fusion': 'graph_fusion',\n",
              "   'val_auc': 0.5898,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3221},\n",
              "  {'fusion': 'invariant_fusion',\n",
              "   'val_auc': 0.5651,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3231},\n",
              "  {'fusion': 'gated_fusion_baseline',\n",
              "   'val_auc': 0.513,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3463},\n",
              "  {'fusion': 'film_conditioning',\n",
              "   'val_auc': 0.4675,\n",
              "   'swap_robustness': 0.5,\n",
              "   'ece': 0.3397}],\n",
              " 'causal': [{'method': 'causal_shap',\n",
              "   'mediation_effect': 0.4476,\n",
              "   'intervention_sensitivity': 0.85},\n",
              "  {'method': 'path_specific_effects',\n",
              "   'mediation_effect': 0.0027,\n",
              "   'intervention_sensitivity': 0.1376},\n",
              "  {'method': 'causal_graph_discovery',\n",
              "   'mediation_effect': 0.0551,\n",
              "   'intervention_sensitivity': 0.0276},\n",
              "  {'method': 'irm_invariance',\n",
              "   'mediation_effect': 0.9886,\n",
              "   'intervention_sensitivity': 0.0114},\n",
              "  {'method': 'mediation_analysis',\n",
              "   'mediation_effect': 0.0035,\n",
              "   'intervention_sensitivity': 0.0028},\n",
              "  {'method': 'do_audio_intervention',\n",
              "   'mediation_effect': 0.0028,\n",
              "   'intervention_sensitivity': 0.0028}]}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def _rank(rows, primary, secondary=None, reverse=True):\n",
        "    def key_fn(item):\n",
        "        p = item.get(primary)\n",
        "        s = item.get(secondary) if secondary else 0.0\n",
        "        if p is None:\n",
        "            return -1e9 if reverse else 1e9\n",
        "        return (p, s if s is not None else 0.0)\n",
        "    return sorted(rows, key=key_fn, reverse=reverse)\n",
        "\n",
        "inventory_rankings = {\n",
        "    \"extraction\": _rank(extraction_results, \"detection_rate\", \"fps\"),\n",
        "    \"preprocessing\": _rank(preprocess_results, \"val_auc\", \"compute_cost\"),\n",
        "    \"fusion\": _rank(fusion_results, \"val_auc\", \"swap_robustness\"),\n",
        "    \"causal\": _rank(causal_results, \"intervention_sensitivity\", \"mediation_effect\"),\n",
        "}\n",
        "\n",
        "inv_path = RESULTS_DIR / \"inventory_rankings.json\"\n",
        "inv_path.write_text(json.dumps(inventory_rankings, indent=2), encoding=\"utf-8\")\n",
        "inventory_rankings\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "CausalX Technique Benchmark",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
